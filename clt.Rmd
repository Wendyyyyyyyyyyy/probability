# Central Limit Theorem {#clt}

## Motivation

In Lesson \@ref(sums-continuous), we saw that calculating the exact distribution of a 
sum of i.i.d. random variables, $S_n = X_1 + X_2 + \ldots + X_n$ is a lot of work. 
Each time we add another random variable, we have to calculate a convolution, which requires 
calculating an integral. For arbitrary p.m.f.s and p.d.f.s, calculating so many convolutions 
is impractical.

However, we often need to calculate probabilities involving the sum of hundreds or even 
thousands of random variables. Since getting the exact distribution of $S_n$ is impractical, 
we need an approximation that is accurate and easy to use. The **Central Limit Theorem** 
provides that approximation.

## Theory

```{theorem clt, name="Central Limit Theorem for Sums"}
Let $X_1, ..., X_n$ be independent and identically distributed (i.i.d.) random variables. 
Then, for $n$ large, their sum
\[ S_n = X_1 + ... + X_n  \]
is approximately normally distributed.
```
Therefore, to calculate a probability such as $P(S_n \leq x)$, we can use a 
normal distribution. Of course, we need to determine the parameters of the 
normal distribution. That is, we need to know $\mu$ and $\sigma$. We choose 
these parameters to match the expectation $E[S_n]$ and the standard deviation $\text{SD}[S_n]$, 
respectively, which we can calculate using properties of expectation and covariance:

\begin{align*}
\mu = E[S_n] &= E[X_1 + X_2 + ... + X_n] \\
&= E[X_1] + E[X_2] + ... + E[X_n] & \text{(linearity of expectation)} \\
&= n E[X_1] & \text{($X_i$s are identically distributed)} \\
\sigma^2 = \text{Var}[S_n] &= \text{Cov}[S_n, S_n] \\
&= \text{Cov}[X_1 + ... + X_n, X_1 + ... + X_n] \\
&= \text{Cov}[X_1, X_1] + \text{Cov}[X_1, X_2] + ... + \text{Cov}[X_n, X_n] & \text{(properties of covariance)} \\
&= \text{Var}[X_1] + ... + \text{Var}[X_n] & \text{($X_i$s are independent)} \\
&= n\text{Var}[X_1] & \text{($X_i$s are identically distributed)} \\
\sigma = \sqrt{\sigma^2} &= \sqrt{n} \text{SD}[X_1].
\end{align*}

Since the average is just the sum divided by $n$, the _average_ of $n$ i.i.d. random variables is 
also approximately normal. (Scaling a normal random variable by a constant yields another 
normal distribution.)
```{theorem clt-means, name="Central Limit Theorem for Means"}
Let $X_1, ..., X_n$ be independent and identically distributed (i.i.d.) random variables. 
Then, for $n$ large, their mean
\[ \bar X_n = \frac{X_1 + ... + X_n}{n}  \]
is approximately normally distributed.
```
The parameters of this normal distribution are:
\begin{align*}
\mu = E[\bar X_n] &= E[\frac{S_n}{n}] \\
&= \frac{n E[X_1]}{n} \\
&= E[X_1] \\
\sigma^2 = \text{Var}[\bar X_n] &= \text{Var}[\frac{S_n}{n}] \\
&= \frac{1}{n^2} \text{Var}[S_n] \\
&= \frac{1}{n^2} n \text{Var}[X_1] \\
&= \frac{\text{Var}[X_1]}{n} \\
\sigma = \text{SD}[\bar X_n] &= \frac{\text{SD}[X_1]}{\sqrt{n}}.
\end{align*}

On average, the mean will be the expected value of one observation, and the uncertainty 
(as measured by the standard deviation) approaches 0 as $n \to \infty$. This is the same
calculation that we did in Lesson \@ref(lln) on the Law of Large Numbers.

## Worked Examples

```{example pyx1, name="Trial of the Pyx"}
Since the 1200s, coins struck by the Royal Mint in England have been evaluated for their metal content in a ceremony called the Trial of the Pyx. This ceremony does not have much meaning today (see the video below), but in the 1700s, English coins were made of gold. The Master of the Mint had an incentive to make coins 
weigh less than the standard, because he could keep the shortfall himself (as long as he was not caught).

<iframe width="560" height="315" src="https://www.youtube.com/embed/UZQfA2cRHJs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  
In the Trial, 100 guineas (i.e., gold coins) would be chosen randomly and independently from all coins 
made at the Mint that year, put in the Pyx (a ceremonial box), and weighed. 
The Master of the Mint was allowed a margin of error, which was set according to the manufacturing 
tolerances of the time.  If the actual weight of the coins in the Pyx differed from its target weight 
by more than this margin on either side, the Master of the Mint was exposed to serious penalties.

 In 1799, _each_ guinea was supposed to weigh 128 grains. Due to manufacturing variability, 
	 the standard deviation of guinea weights was about 0.1 grains. To give the 
	 Master of the Mint some wiggle room, the allowable margin of error for _each_
	 guinea was set at 0.32 grains.
	  
Let's suppose the Master of the Mint makes the guineas weigh $127.7$ grains on average, skimming 
$0.1$ grains from each coin. What is the probability he gets caught?
```

```{solution}
Let $X_i$ be the actual weight of a single guinea. Then, $E[X_i] = 127.7$ and 
			$\text{Var}[X_i] = 0.1^2$. The total weight of the 100 guineas is:
			\[ S = X_1 + ... + X_{100}. \]

We know that 
    \begin{align*}
    E[S] &= 100 \cdot 127.7 = 12770 & \text{Var}[S] &= 100 \cdot 0.1^2 \\
    & & \text{SD}[S] &= \sqrt{100} \cdot 0.1 = 1
    \end{align*}

By the Central Limit Theorem, we can approximate the distribution of $S$ by 
			\[ S \approx \textrm{Normal}(\mu=12770, \sigma=1). \]
			Notice that we did not need to know how the weights of 
			individual guineas are distributed! The weight of an individual guinea can follow any 
			distribution, and the _total_ weight of 100 independent guineas will still follow 
			this normal distribution.
			
Now, the Master of the Mint fails the Trial if the weight is off by 32 grains. That is, 
			he fails if $S < 12800 - 32$. This probability is
			\[ P(S < 12768) = P(\frac{S - 12770}{1} < \frac{12768 - 12770}{1}) = \Phi(-2) \approx .023. \]
			(Technically, the Master also fails the Trial if the weight is off by 
			  more than 32 in the _other_ direction (i.e., $S > 12832$), 
			  but this is probability is so small that we can ignore it.)
```

```{example pyx2, name="Trial of the Pyx continued"}
Not satisfied with a 1-in-50 chance of being caught, the Master of the Mint 
wants to reduce his chances of getting caught to 0.1\%? 
  What should he make the guineas weigh on average?
```{solution}
Let $X_i$ denote the weight of a single guinea. Then, $E[X_i] = m$ (we are trying to determine 
                                                                       this) and 
$\text{Var}[X_i] = 0.1^2$. The total weight of the 100 guineas is:
			\[ S = X_1 + ... +X_{100}. \]

By the same logic as in Example \@ref(exm:pyx1), $S \approx \textrm{Normal}(\mu=100m, \sigma=1)$. Our goal 
is to solve for $m$ so that 
\[ P(S < 12768) = .001. \]

To this end, we standardize both sides:
\[ .001 = P(\frac{S - 100m}{1} < \frac{12768 - 100m}{1}) = \Phi(12768 - 100m). \]
Using the quantile function, we find that 
\[ 12768 - 100m \approx -3.09, \]
so he would just need to target an average weight of $m \approx 127.71$ grains, which is only a 
smidge higher (and still quite far off from the target weight of 128 grains).
```

Where did the government go wrong in setting up the Trial of the Pyx? They were much too 
lenient in allowing the Master of the Mint a margin of 32 grains. We know that the 
uncertainty in the sum of $n$ independent random variables grows as $\sqrt{n}$, not $n$. 
So if they wanted to 
allow the Master of the Mint a margin of 0.32 grains per coin, they should have 
only allowed him a margin of $\sqrt{100} \cdot 32 = 3.2$ grains for 100 coins. Instead, 
they allowed him a margin that was 10 times larger.

```{example normal-approx-binom, name="Normal Approximation to the Binomial"}
The Central Limit Theorem works for discrete random variables as well. 
For example, suppose a casino suspects that one of its roulette wheels is defective. 
They notice that the ball has landed in the 0 or 00 pockets 22 times in the last 300 spins. 
Is this evidence that the roulette wheel is biased towards 0 and 00?
  
We can set up a box model for the _number of times that the ball lands in the 0 or 00 pockets_. We
can represent this as the number of $\fbox{1}$s in $n=300$ draws (with replacement) from the box
\[ \fbox{$\underbrace{\fbox{1}\ \fbox{1}}_{N_1=2}\ \underbrace{\fbox{0}\ \fbox{0}\ \ldots\ \fbox{0}\ \fbox{0}}_{N_0=36}$}. \]
This is the description of a $\text{Binomial}(n=300, p=2/38)$ random variable. Using a binomial 
distribution calculator, the (exact) probability of getting 22 or more is 
```{python}
# Python code
from symbulate import *
1 - Binomial(n=300, p=2/38).cdf(21)
```
```{r}
# R code
1 - pbinom(21, size=300, prob=2/38)
```

We can also approximate this random variable---let's call it $S$---by a normal distribution. 
To see this, write 
\[ S = X_1 + X_2 + \ldots + X_{300}, \]
where each $X_i$ represents the outcome of one draw. Because the draws are made with 
replacement, the $X_i$s are independent. By the Central Limit Theorem, $S$ is approximately normal.

We approximate it by a normal distribution with the right expected value and standard deviation:
\begin{align*}
E[S] &= np = 300 \cdot \frac{2}{38} \approx 15.79 & \text{SD}[S] &= \sqrt{np(1-p)} = \sqrt{300 \cdot \frac{2}{38} \cdot (1 - \frac{2}{38})} \approx 3.87.
\end{align*}
So $S \approx \text{Normal}(\mu=10.53, \sigma=3.15)$. Now we can calculate $P(S \geq 22)$ using a normal 
distribution calculator:
```{python}
# Python code
1 - Normal(mean=300 * 2/38, sd=sqrt(300 * 2/38 * (1 - 2/38))).cdf(22)
```
```{r}
# R
1 - pnorm(22, mean=300 * 2/38, sd=sqrt(300 * 2/38 * (1 - 2/38)))
```

Notice that we plugged in 22 into the c.d.f. because the normal distribution is continuous. That is,
\[ P(S \geq 22) = 1 - P(S < 22) = 1 - P(S \leq 22), \]
since the probability that $S$ equals 22 exactly is zero. However, because
we are using the normal distribution to approximate a discrete distribution, 
some people have argued that we should plug in 21 or 21.5 instead.

In my opinion, it is a waste of time to worry about such details, since this is no longer a 
practical method of calculating probabilities. There is no reason to use 
the normal approximation to calculate this probability when exact binomial calculators 
are available. The normal approximation to the binomial is presented primarily as an 
application of the Central Limit Theorem.

Shown below is the normal approximation (in red) to the binomial p.m.f. (in black). 
As we can see, the normal approximation is good, but not perfect.

```{r normal-approx-binomial, echo=FALSE, fig.show = "hold", fig.align = "default", fig.asp=0.7}
plot(0:50, dbinom(0:50, 300, 2/38), pch=19, xlab="x", ylab="f(x)", xlim=c(0, 45))
for(x in 0:50) {
  lines(c(x, x), c(0, dbinom(x, 300, 2/38)))
}

x <- seq(0, 50, by=.1)
lines(x, dnorm(x, mean=15.79, sd=3.87), col="red", lwd=2)
```


## Essential Practice {-}

1.  There are 40 students in an elementary statistics class. From years of experience, the
instructor knows that the time needed to grade a randomly chosen paper from an exam is a
random variable with an expected value of 6 min and a standard deviation of 6 min.

    If grading times are independent and the instructor begins grading at 8:00 p.m. and grades
continuously, what is the (approximate) probability that she is done grading before the late 
night shows begin at 11:30 p.m.?

2. In roulette, a bet on a single number has a $1/38$ probability of success and pays 35-to-1. 
Consider betting on a single number on each of $n$ spins of a roulette wheel. Let 
$\bar X_n$ be your average net winnings over those $n$ bets. 

    a. For which of the following values of $n$ is $\bar X_n$ reasonably close to normally 
    distributed? Do a simulation to find out. (Copy the code below into a Colab and modify.)
    
        ```{python, results="hide"}
        # Python code (Make sure Symbulate has been imported.) 
        n = 10
        P = BoxModel({35: 1, -1: 37}, size=n, replace=True)
        X = RV(P, mean)
        X.sim(9999)
        # You can use .plot() to make a histogram of these results.
        ```
        
        ```{r, results="hide"}
        # R code
        n <- 10
        replicate(9999,
                  sum(sample(c(35, rep(-1, 37)), size=n, replace=TRUE)))
        # You can use hist() to make a histogram of these results.
        ```
    
        - $n=10$
        - $n=100$
        - $n=1000$
        - $n=10000$
    
    b. For each $n$, calculate the approximate probability that you come out ahead, 
    i.e., $P(\bar X_n > 0)$. I encourage you to use a combination of simulation and the 
    Central Limit Theorem (provided that it applies).
    
    c. The casino wants to determine how many bets on a single number they need 
    in order to have at least a 99% probability of coming out ahead. (Remember, the casino 
    come out ahead if you lose, i.e., $\bar X_n < 0$.) Determine the minimum number of 
    bets, and remember that $n$ must be an integer.

3. In San Luis Obispo, radioactive particles reach a Geiger counter according to a Poisson process 
at a rate of $\lambda = 0.8$ particles per second. What is the probability that the 100th particle 
is detected between 90 and 120 seconds? Calculate this probability in two ways:

    a. Make an argument for why the time the 100th particle arrives should be approximately normal. 
    Use the normal approximation to calculate the approximate probability.
    b. We derived the exact p.d.f. of the $r$th arrival time in Example \@ref(exm:pp-arrival-dist). 
    Integrate this p.d.f. to get the exact probability.
    c. How good was the normal approximation?
