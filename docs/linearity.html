<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lesson 26 Linearity of Expectation | Introduction to Probability</title>
  <meta name="description" content="Introduction to probability textbook." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Lesson 26 Linearity of Expectation | Introduction to Probability" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Introduction to probability textbook." />
  <meta name="github-repo" content="dlsun/probability" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lesson 26 Linearity of Expectation | Introduction to Probability" />
  
  <meta name="twitter:description" content="Introduction to probability textbook." />
  

<meta name="author" content="Dennis Sun" />


<meta name="date" content="2020-07-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lotus2d.html"/>
<link rel="next" href="ev-product.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Probability</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="counting.html"><a href="counting.html"><i class="fa fa-check"></i><b>1</b> Probability and Counting</a><ul>
<li class="chapter" data-level="" data-path="counting.html"><a href="counting.html#motivating-example"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="counting.html"><a href="counting.html#theory"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="counting.html"><a href="counting.html#examples"><i class="fa fa-check"></i>Examples</a></li>
<li class="chapter" data-level="" data-path="counting.html"><a href="counting.html#additional-exercises"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="factorial.html"><a href="factorial.html"><i class="fa fa-check"></i><b>2</b> The Factorial</a><ul>
<li class="chapter" data-level="" data-path="factorial.html"><a href="factorial.html#motivating-example-1"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="factorial.html"><a href="factorial.html#theory-1"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="factorial.html"><a href="factorial.html#examples-1"><i class="fa fa-check"></i>Examples</a></li>
<li class="chapter" data-level="" data-path="factorial.html"><a href="factorial.html#additional-exercises-1"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="box-models.html"><a href="box-models.html"><i class="fa fa-check"></i><b>3</b> Box Models and Combinations</a><ul>
<li class="chapter" data-level="" data-path="box-models.html"><a href="box-models.html#motivating-example-2"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="box-models.html"><a href="box-models.html#theory-2"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="box-models.html"><a href="box-models.html#essential-practice"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="box-models.html"><a href="box-models.html#additional-practice"><i class="fa fa-check"></i>Additional Practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="replacement.html"><a href="replacement.html"><i class="fa fa-check"></i><b>4</b> Sampling With Replacement</a><ul>
<li class="chapter" data-level="" data-path="replacement.html"><a href="replacement.html#motivating-example-3"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="replacement.html"><a href="replacement.html#discussion"><i class="fa fa-check"></i>Discussion</a></li>
<li class="chapter" data-level="" data-path="replacement.html"><a href="replacement.html#examples-2"><i class="fa fa-check"></i>Examples</a></li>
<li class="chapter" data-level="" data-path="replacement.html"><a href="replacement.html#bonus-material"><i class="fa fa-check"></i>Bonus Material</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="double-counting.html"><a href="double-counting.html"><i class="fa fa-check"></i><b>5</b> Double Counting</a><ul>
<li class="chapter" data-level="" data-path="double-counting.html"><a href="double-counting.html#motivating-example-4"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="double-counting.html"><a href="double-counting.html#theory-3"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="double-counting.html"><a href="double-counting.html#examples-3"><i class="fa fa-check"></i>Examples</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conditional.html"><a href="conditional.html"><i class="fa fa-check"></i><b>6</b> Conditional Probability</a><ul>
<li class="chapter" data-level="" data-path="conditional.html"><a href="conditional.html#motivating-example-5"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="conditional.html"><a href="conditional.html#theory-4"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="conditional.html"><a href="conditional.html#examples-4"><i class="fa fa-check"></i>Examples</a></li>
<li class="chapter" data-level="" data-path="conditional.html"><a href="conditional.html#additional-exercises-2"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="independence.html"><a href="independence.html"><i class="fa fa-check"></i><b>7</b> Independence</a><ul>
<li class="chapter" data-level="" data-path="independence.html"><a href="independence.html#motivating-example-6"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="independence.html"><a href="independence.html#theory-5"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="independence.html"><a href="independence.html#examples-5"><i class="fa fa-check"></i>Examples</a></li>
<li class="chapter" data-level="" data-path="independence.html"><a href="independence.html#additional-exercises-3"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ltp.html"><a href="ltp.html"><i class="fa fa-check"></i><b>8</b> Law of Total Probability</a><ul>
<li class="chapter" data-level="" data-path="ltp.html"><a href="ltp.html#motivating-example-7"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="ltp.html"><a href="ltp.html#theory-6"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="ltp.html"><a href="ltp.html#examples-6"><i class="fa fa-check"></i>Examples</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>9</b> Bayes’ Theorem</a><ul>
<li class="chapter" data-level="9.1" data-path="bayes.html"><a href="bayes.html#motivating-example-8"><i class="fa fa-check"></i><b>9.1</b> Motivating Example</a></li>
<li class="chapter" data-level="9.2" data-path="bayes.html"><a href="bayes.html#theory-7"><i class="fa fa-check"></i><b>9.2</b> Theory</a></li>
<li class="chapter" data-level="9.3" data-path="bayes.html"><a href="bayes.html#examples-7"><i class="fa fa-check"></i><b>9.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rv.html"><a href="rv.html"><i class="fa fa-check"></i><b>10</b> Random Variables</a><ul>
<li class="chapter" data-level="" data-path="rv.html"><a href="rv.html#motivating-example-9"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="rv.html"><a href="rv.html#theory-8"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="rv.html"><a href="rv.html#essential-practice-1"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="rv.html"><a href="rv.html#additional-exercises-4"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="cdf.html"><a href="cdf.html"><i class="fa fa-check"></i><b>11</b> Cumulative Distribution Functions</a><ul>
<li class="chapter" data-level="" data-path="cdf.html"><a href="cdf.html#theory-9"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="cdf.html"><a href="cdf.html#examples-8"><i class="fa fa-check"></i>Examples</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="hypergeometric.html"><a href="hypergeometric.html"><i class="fa fa-check"></i><b>12</b> Hypergeometric Distribution</a><ul>
<li class="chapter" data-level="" data-path="hypergeometric.html"><a href="hypergeometric.html#motivating-example-10"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="hypergeometric.html"><a href="hypergeometric.html#theory-10"><i class="fa fa-check"></i>Theory</a><ul>
<li class="chapter" data-level="" data-path="hypergeometric.html"><a href="hypergeometric.html#visualizing-the-distribution"><i class="fa fa-check"></i>Visualizing the Distribution</a></li>
<li class="chapter" data-level="" data-path="hypergeometric.html"><a href="hypergeometric.html#calculating-hypergeometric-probabilities-on-the-computer"><i class="fa fa-check"></i>Calculating Hypergeometric Probabilities on the Computer</a></li>
<li class="chapter" data-level="" data-path="hypergeometric.html"><a href="hypergeometric.html#another-formula-for-the-hypergeometric-distribution-optional"><i class="fa fa-check"></i>Another Formula for the Hypergeometric Distribution (optional)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypergeometric.html"><a href="hypergeometric.html#essential-practice-2"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="hypergeometric.html"><a href="hypergeometric.html#additional-exercises-5"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="binomial.html"><a href="binomial.html"><i class="fa fa-check"></i><b>13</b> Binomial Distribution</a><ul>
<li class="chapter" data-level="" data-path="binomial.html"><a href="binomial.html#motivating-example-11"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="binomial.html"><a href="binomial.html#theory-11"><i class="fa fa-check"></i>Theory</a><ul>
<li class="chapter" data-level="" data-path="binomial.html"><a href="binomial.html#visualizing-the-distribution-1"><i class="fa fa-check"></i>Visualizing the Distribution</a></li>
<li class="chapter" data-level="" data-path="binomial.html"><a href="binomial.html#calculating-binomial-probabilities-on-the-computer"><i class="fa fa-check"></i>Calculating Binomial Probabilities on the Computer</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="binomial.html"><a href="binomial.html#essential-practice-3"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="binomial.html"><a href="binomial.html#additional-exercises-6"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="geometric.html"><a href="geometric.html"><i class="fa fa-check"></i><b>14</b> Geometric Distribution</a><ul>
<li class="chapter" data-level="" data-path="geometric.html"><a href="geometric.html#motivating-example-12"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="geometric.html"><a href="geometric.html#theory-12"><i class="fa fa-check"></i>Theory</a><ul>
<li class="chapter" data-level="" data-path="geometric.html"><a href="geometric.html#visualizing-the-distribution-2"><i class="fa fa-check"></i>Visualizing the Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="geometric.html"><a href="geometric.html#essential-practice-4"><i class="fa fa-check"></i>Essential Practice</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="negative-binomial.html"><a href="negative-binomial.html"><i class="fa fa-check"></i><b>15</b> Negative Binomial Distribution</a><ul>
<li class="chapter" data-level="" data-path="negative-binomial.html"><a href="negative-binomial.html#motivating-example-13"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="negative-binomial.html"><a href="negative-binomial.html#theory-13"><i class="fa fa-check"></i>Theory</a><ul>
<li class="chapter" data-level="" data-path="negative-binomial.html"><a href="negative-binomial.html#visualizing-the-distribution-3"><i class="fa fa-check"></i>Visualizing the Distribution</a></li>
<li class="chapter" data-level="" data-path="negative-binomial.html"><a href="negative-binomial.html#calculating-negative-binomial-probabilities-on-the-computer"><i class="fa fa-check"></i>Calculating Negative Binomial Probabilities on the Computer</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="negative-binomial.html"><a href="negative-binomial.html#essential-practice-5"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="negative-binomial.html"><a href="negative-binomial.html#additional-exercises-7"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="poisson.html"><a href="poisson.html"><i class="fa fa-check"></i><b>16</b> Poisson Distribution</a><ul>
<li class="chapter" data-level="" data-path="poisson.html"><a href="poisson.html#motivating-example-14"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="poisson.html"><a href="poisson.html#theory-14"><i class="fa fa-check"></i>Theory</a><ul>
<li class="chapter" data-level="" data-path="poisson.html"><a href="poisson.html#visualizing-the-distribution-4"><i class="fa fa-check"></i>Visualizing the Distribution</a></li>
<li class="chapter" data-level="" data-path="poisson.html"><a href="poisson.html#calculating-poisson-probabilities-on-the-computer"><i class="fa fa-check"></i>Calculating Poisson Probabilities on the Computer</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="poisson.html"><a href="poisson.html#essential-practice-6"><i class="fa fa-check"></i>Essential Practice</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="poisson-process.html"><a href="poisson-process.html"><i class="fa fa-check"></i><b>17</b> Poisson Process</a><ul>
<li class="chapter" data-level="" data-path="poisson-process.html"><a href="poisson-process.html#motivating-example-15"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="poisson-process.html"><a href="poisson-process.html#theory-15"><i class="fa fa-check"></i>Theory</a><ul>
<li class="chapter" data-level="" data-path="poisson-process.html"><a href="poisson-process.html#why-poisson"><i class="fa fa-check"></i>Why Poisson?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="poisson-process.html"><a href="poisson-process.html#essential-practice-7"><i class="fa fa-check"></i>Essential Practice</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="joint-discrete.html"><a href="joint-discrete.html"><i class="fa fa-check"></i><b>18</b> Joint Distributions</a><ul>
<li class="chapter" data-level="" data-path="joint-discrete.html"><a href="joint-discrete.html#motivating-example-16"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="joint-discrete.html"><a href="joint-discrete.html#theory-16"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="joint-discrete.html"><a href="joint-discrete.html#essential-practice-8"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="joint-discrete.html"><a href="joint-discrete.html#additional-exercises-8"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="marginal-discrete.html"><a href="marginal-discrete.html"><i class="fa fa-check"></i><b>19</b> Marginal Distributions</a><ul>
<li class="chapter" data-level="" data-path="marginal-discrete.html"><a href="marginal-discrete.html#motivating-example-17"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="marginal-discrete.html"><a href="marginal-discrete.html#theory-17"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="marginal-discrete.html"><a href="marginal-discrete.html#essential-practice-9"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="marginal-discrete.html"><a href="marginal-discrete.html#additional-exercises-9"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="conditional-discrete.html"><a href="conditional-discrete.html"><i class="fa fa-check"></i><b>20</b> Conditional Distributions</a><ul>
<li class="chapter" data-level="" data-path="conditional-discrete.html"><a href="conditional-discrete.html#motivating-example-18"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="conditional-discrete.html"><a href="conditional-discrete.html#theory-18"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="conditional-discrete.html"><a href="conditional-discrete.html#essential-practice-10"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="conditional-discrete.html"><a href="conditional-discrete.html#additional-exercises-10"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="sums-discrete.html"><a href="sums-discrete.html"><i class="fa fa-check"></i><b>21</b> Sums of Random Variables</a><ul>
<li class="chapter" data-level="" data-path="sums-discrete.html"><a href="sums-discrete.html#theory-19"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="sums-discrete.html"><a href="sums-discrete.html#essential-practice-11"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="sums-discrete.html"><a href="sums-discrete.html#additional-exercises-11"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="expected-value.html"><a href="expected-value.html"><i class="fa fa-check"></i><b>22</b> Expected Value</a><ul>
<li class="chapter" data-level="" data-path="expected-value.html"><a href="expected-value.html#motivating-example-19"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="expected-value.html"><a href="expected-value.html#theory-20"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="expected-value.html"><a href="expected-value.html#essential-practice-12"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="expected-value.html"><a href="expected-value.html#additional-exercises-12"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="ev-infinity.html"><a href="ev-infinity.html"><i class="fa fa-check"></i><b>23</b> Expected Value and Infinity</a><ul>
<li class="chapter" data-level="23.1" data-path="ev-infinity.html"><a href="ev-infinity.html#pascals-wager"><i class="fa fa-check"></i><b>23.1</b> Pascal’s Wager</a></li>
<li class="chapter" data-level="23.2" data-path="ev-infinity.html"><a href="ev-infinity.html#st-petersburg"><i class="fa fa-check"></i><b>23.2</b> St. Petersburg Paradox</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="lotus.html"><a href="lotus.html"><i class="fa fa-check"></i><b>24</b> LOTUS</a><ul>
<li class="chapter" data-level="" data-path="lotus.html"><a href="lotus.html#motivating-example-20"><i class="fa fa-check"></i>Motivating Example</a></li>
<li class="chapter" data-level="" data-path="lotus.html"><a href="lotus.html#theory-21"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="lotus.html"><a href="lotus.html#essential-practice-13"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="lotus.html"><a href="lotus.html#additional-exercises-13"><i class="fa fa-check"></i>Additional Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="lotus2d.html"><a href="lotus2d.html"><i class="fa fa-check"></i><b>25</b> 2D LOTUS</a><ul>
<li class="chapter" data-level="" data-path="lotus2d.html"><a href="lotus2d.html#theory-22"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="lotus2d.html"><a href="lotus2d.html#essential-practice-14"><i class="fa fa-check"></i>Essential Practice</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="linearity.html"><a href="linearity.html"><i class="fa fa-check"></i><b>26</b> Linearity of Expectation</a><ul>
<li class="chapter" data-level="" data-path="linearity.html"><a href="linearity.html#theory-23"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="linearity.html"><a href="linearity.html#essential-practice-15"><i class="fa fa-check"></i>Essential Practice</a></li>
<li class="chapter" data-level="" data-path="linearity.html"><a href="linearity.html#additional-practice-1"><i class="fa fa-check"></i>Additional Practice</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="ev-product.html"><a href="ev-product.html"><i class="fa fa-check"></i><b>27</b> Expected Value of a Product</a><ul>
<li class="chapter" data-level="" data-path="ev-product.html"><a href="ev-product.html#theory-24"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="ev-product.html"><a href="ev-product.html#essential-practice-16"><i class="fa fa-check"></i>Essential Practice</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Probability</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linearity" class="section level1">
<h1><span class="header-section-number">Lesson 26</span> Linearity of Expectation</h1>
<div id="theory-23" class="section level2 unnumbered">
<h2>Theory</h2>
<p>In this lesson, we learn a shortcut for calculating expected values of the form
<span class="math display">\[ E[aX + bY]. \]</span>
In general, evaluating expected values of <em>functions</em> of random variables
requires LOTUS. But when the function is linear, we can break up the expected
value into more manageable parts.</p>

<div class="theorem">
<span id="thm:ev-constant" class="theorem"><strong>Theorem 26.1  (Adding or Multiplying a Constant)  </strong></span>Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(a, b\)</span> be constants. Then,
<span class="math display" id="eq:ev-add" id="eq:ev-multiply">\[\begin{align}
E[aX] &amp;= aE[X] \tag{26.1} \\
E[X + b] &amp;= E[X] + b \tag{26.2}
\end{align}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Since <span class="math inline">\(aX\)</span> and <span class="math inline">\(X+b\)</span> are technically functions of <span class="math inline">\(X\)</span>, we use
LOTUS <a href="lotus.html#eq:lotus">(24.1)</a>.
<span class="math display">\[\begin{align*}
E[aX] &amp;= \sum_x ax f(x) &amp; \text{(LOTUS)} \\
&amp;= a \sum_x x f(x) &amp; \text{(factor constant outside the sum)} \\
&amp;= a E[X] &amp; \text{(definition of expected value)}. \\
E[X+b] &amp;= \sum_x (x + b) f(x) &amp; \text{(LOTUS)} \\
&amp;= \sum_x x f(x) + \sum_x b f(x) &amp; \text{(break $(x + b) f(x)$ into $xf(x) + bf(x)$)} \\
&amp;= \sum_x x f(x) + b\sum_x f(x) &amp; \text{(factor constant outside the sum)} \\
&amp;= E[X] + b &amp; \text{(definitions of expected value, p.m.f.)}.
\end{align*}\]</span>
</div>

<p>Here is an example illustrating how Theorem <a href="linearity.html#thm:ev-constant">26.1</a> can be used.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-91" class="example"><strong>Example 26.1  (Expected Values in Roulette)  </strong></span>In roulette, betting on a single number pays 35-to-1. That is, for each $1 you bet, you win
$35 if the ball lands in that pocket.</p>
<p>If we let <span class="math inline">\(X\)</span> represent your net winnings (or losses) on this bet, its p.m.f. is
<span class="math display">\[ \begin{array}{r|cc} x &amp; -1 &amp; 35 \\ \hline f_X(x) &amp; 37/38 &amp; 1/38 \end{array}. \]</span></p>
<p>In Lesson <a href="expected-value.html#expected-value">22</a>, we calculated <span class="math inline">\(E[X]\)</span> directly. Here is another way
we can calculate it using Theorem <a href="linearity.html#thm:ev-constant">26.1</a>. Let us define a new random variable
<span class="math inline">\(W\)</span>, which takes on the values 0 and 1 with the same probabilities:
<span class="math display">\[ \begin{array}{r|cc} w &amp; 0 &amp; 1 \\ \hline f_W(w) &amp; 37/38 &amp; 1/38 \end{array}. \]</span>
We can think of <span class="math inline">\(W\)</span> as an <em>indicator</em> variable for whether or not we win.
It is easy to see that <span class="math inline">\(E[W] = 1/38\)</span>. (One way is to just use the formula. Another is
to note that <span class="math inline">\(W\)</span> is a <span class="math inline">\(\text{Binomial}(n=1, p=1/38)\)</span> random variable, so <span class="math inline">\(E[W] = np = 1/38\)</span>.)</p>
Now, the amount we win, <span class="math inline">\(X\)</span>, is related to this indicator variable, <span class="math inline">\(W\)</span>, by:
<span class="math display">\[ X = 36 W - 1. \]</span>
(Verify that <span class="math inline">\(X\)</span> takes on the values <span class="math inline">\(35\)</span> and <span class="math inline">\(-1\)</span> with the correct probabilities.)
Now, by Theorem <a href="linearity.html#thm:ev-constant">26.1</a>, the expected value is
<span class="math display">\[ E[X] = E[36W - 1] = 36 E[W] - 1 = 36 \left( \frac{1}{38} \right) - 1 = -\frac{2}{38}, \]</span>
which matches what we got in Lesson <a href="expected-value.html#expected-value">22</a>.
</div>

<p>The next result is even more useful.</p>

<div class="theorem">
<span id="thm:linearity" class="theorem"><strong>Theorem 26.2  (Linearity of Expectation)  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables. Then, <em>no matter what their joint distribution is</em>,
<span class="math display" id="eq:linearity">\[\begin{equation}
E[X+Y] = E[X] + E[Y].
\tag{26.3}
\end{equation}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Since <span class="math inline">\(E[X + Y]\)</span> involves two random variables, we have to evaluate the expectation
using 2D LOTUS <a href="lotus2d.html#eq:lotus2d">(25.1)</a>, with <span class="math inline">\(g(x, y) = x + y\)</span>. Suppose
that the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math inline">\(f(x, y)\)</span>. Then:
<span class="math display">\[\begin{align*}
E[X + Y] &amp;= \sum_x \sum_y (x + y) f(x, y) &amp; \text{(2D LOTUS)} \\
&amp;= \sum_x \sum_y x f(x, y) + \sum_x \sum_y y f(x, y) &amp; \text{(break $(x + y) f(x, y)$ into $x f(x, y) + y f(x, y)$)} \\
&amp;= \sum_x x \sum_y f(x, y) + \sum_y y \sum_x f(x, y) &amp; \text{(move term outside the inner sum)} \\
&amp;= \sum_x x f_X(x) + \sum_y y f_Y(y) &amp; \text{(definition of marginal distribution)} \\
&amp;= E[X] + E[Y] &amp; \text{(definition of expected value)}.
\end{align*}\]</span></p>
</div>

<p>In other words, linearity of expectation says that you only need to know the
marginal distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to calculate <span class="math inline">\(E[X + Y]\)</span>. Their joint distribution
is irrelevant.</p>
<p>Let’s apply this to the Xavier and Yolanda problem from Lesson <a href="joint-discrete.html#joint-discrete">18</a>.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-93" class="example"><strong>Example 26.2  (Xavier and Yolanda Revisited)  </strong></span>Xavier and Yolanda head to the roulette table at a casino. They both place bets on red on 3 spins of the roulette
wheel before Xavier has to leave. After Xavier leaves, Yolanda places bets on red on 2 more spins of the wheel.
Let <span class="math inline">\(X\)</span> be the number of bets that Xavier wins and <span class="math inline">\(Y\)</span> be the number that Yolanda wins.</p>
<p>In Lesson <a href="lotus2d.html#lotus2d">25</a>, we calculated <span class="math inline">\(E[Y - X]\)</span>, the expected number of <em>additional</em> times that Yolanda wins,
by applying 2D LOTUS to the joint p.m.f. of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The calculation was tedious.</p>
<p>In this lesson, we see how linearity of expectation allows us to avoid tedious calculations. First,
by <a href="linearity.html#eq:ev-multiply">(26.1)</a> and <a href="linearity.html#eq:linearity">(26.3)</a>, we see that:
<span class="math display">\[ E[Y - X] = E[Y] + E[-1 \cdot X] = E[Y] + (-1) E[X] = E[Y] - E[X]. \]</span>
We know that <span class="math inline">\(Y\)</span> is <span class="math inline">\(\text{Binomial}(n=5, N_1=18, N_0=20)\)</span> and <span class="math inline">\(X\)</span> is <span class="math inline">\(\text{Binomial}(n=3, N_1=18, N_0=20)\)</span>.
<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are definitely not independent, since three of Yolanda’s bets are identical to Xavier’s.
But linearity of expectation says that to calculate <span class="math inline">\(E[Y - X]\)</span>, it does not matter how <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are
related to each other; we only need their marginal distributions.</p>
From Lesson <a href="expected-value.html#expected-value">22</a>, we know the expected value of a binomial random variable is <span class="math inline">\(n\frac{N_1}{N}\)</span>, so
<span class="math display">\[ E[Y - X] = E[Y] - E[X] = 5\frac{18}{38} - 3\frac{18}{38} = 2\frac{18}{38} \approx .947, \]</span>
which matches the answer we got in Lesson <a href="lotus2d.html#lotus2d">25</a> by applying 2D LOTUS.
</div>

<p>Linearity allows us to calculate the expected values of complicated random variables
by breaking them into simpler random variables.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-94" class="example"><strong>Example 26.3  (Expected Value of the Binomial and Hypergeometric Distributions)  </strong></span>In Lesson <a href="expected-value.html#expected-value">22</a>, we showed that the expected values of the binomial and hypergeometric
distributions are the same: <span class="math inline">\(n\frac{N_1}{N}\)</span>. But the proofs we gave were tedious and did not give any insight
into why this formula is true. Let’s prove this formula using linearity of expectation.</p>
<p>If <span class="math inline">\(X\)</span> is a <span class="math inline">\(\text{Binomial}(n, N_1, N_0)\)</span> random variable, then we can break <span class="math inline">\(X\)</span> down into the sum
of simpler random variables:
<span class="math display">\[ X = Y_1 + Y_2 + \ldots + Y_n, \]</span>
where <span class="math inline">\(Y_i\)</span> represents the outcome of the <span class="math inline">\(i\)</span>th draw from the box. So <span class="math inline">\(Y_i\)</span> equals <span class="math inline">\(1\)</span> with probability
<span class="math inline">\(N_1/N\)</span> and is <span class="math inline">\(0\)</span> otherwise. Its p.m.f. is about as simple as it gets:
<span class="math display">\[\begin{equation}
\begin{array}{rcc}
y &amp; 0 &amp; 1 \\
\hline
f(y) &amp; N_0/N &amp; N_1/N
\end{array}. 
\label{eq:bernoulli_pmf}
\end{equation}\]</span></p>
<p>By linearity of expectation:
<span class="math display">\[ E[X] = E[Y_1] + E[Y_2] + \ldots + E[Y_n]. \]</span>
We have taken a complicated random variable <span class="math inline">\(X\)</span> and broken it down into simpler random variables
<span class="math inline">\(Y_i\)</span>, whose expected value is trivial to calculate:
<span class="math display">\[ E[Y_i] = 0 \cdot \frac{N_0}{N} + 1 \cdot \frac{N_1}{N} = \frac{N_1}{N}.  \]</span>
Therefore,
<span class="math display">\[ E[X] = \underbrace{\frac{N_1}{N} + \frac{N_1}{N} + \ldots + \frac{N_1}{N}}_{\text{$n$ terms}} = n \frac{N_1}{N}. \]</span></p>
What if <span class="math inline">\(X\)</span> is a <span class="math inline">\(\text{Hypergeometric}(n, N_1, N_0)\)</span> random variable? We can break <span class="math inline">\(X\)</span> down in exactly the same way, as
sum of the outcomes of each draw:
<span class="math display">\[ X = Y_1 + Y_2 + \ldots + Y_n, \]</span>
except that now the <span class="math inline">\(Y_i\)</span>s are not independent. However, each <span class="math inline">\(Y_i\)</span> still represents a random draw from a box with
<span class="math inline">\(N_1\)</span> <span class="math inline">\(\fbox{1}\)</span>s and <span class="math inline">\(N_0\)</span> <span class="math inline">\(\fbox{0}\)</span>s, so <span class="math inline">\(Y_i\)</span> equals 1 with probability <span class="math inline">\(N_1/N\)</span>, just as before.<br />
Also, linearity of expectation does not care whether or not the random variables are independent. So the expected value of the
hypergeometric is also:
<span class="math display">\[ E[X] = \underbrace{\frac{N_1}{N} + \frac{N_1}{N} + \ldots + \frac{N_1}{N}}_{\text{$n$ terms}} = n \frac{N_1}{N}. \]</span>
</div>

<p>Here is a particularly clever application of linearity.</p>

<div class="example">
<p><span id="exm:binomial-lotus-2" class="example"><strong>Example 26.4  </strong></span>Let <span class="math inline">\(X\)</span> be a <span class="math inline">\(\text{Binomial}(n, N_1, N_0)\)</span> random variable. What is <span class="math inline">\(E[X(X-1)]\)</span>?
In Example <a href="lotus.html#exm:binomial-lotus">24.3</a>, we calculated this expected value using LOTUS. Here is a
way to calculate it using linearity.</p>
<p>Remember that <span class="math inline">\(X\)</span> represents the number of <span class="math inline">\(\fbox{1}\)</span>s in our sample.
The random variable <span class="math inline">\(X(X-1)\)</span> then represents the number of (ordered) ways to choose
two tickets from the <span class="math inline">\(\fbox{1}\)</span>s in our sample. In the diagram below,
<span class="math inline">\(n=4\)</span> and <span class="math inline">\(X=3\)</span>. Each arrow represents one of the <span class="math inline">\(n(n-1) = 12\)</span> ways to choose
two tickets from the <span class="math inline">\(n\)</span> tickets in the sample. The red arrows represent
the <span class="math inline">\(X(X-1) = 6\)</span> ways of choosing two tickets among the <span class="math inline">\(\fbox{1}\)</span>s.</p>
</div>

<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-95-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Let’s define an indicator variable <span class="math inline">\(Y_{ij}, i\neq j\)</span> for each of the <span class="math inline">\(n(n-1)\)</span>
ways of choosing two tickets from our sample. Let <span class="math inline">\(Y_{ij}\)</span> be 1 if tickets <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>
are both <span class="math inline">\(\fbox{1}\)</span>s. In other words, <span class="math inline">\(Y_{ij} = 1\)</span> if and only if there is a red arrow
connecting the two tickets in the diagram above.</p>
<p>Since <span class="math inline">\(X(X-1)\)</span> is the number of red arrows, we have
<span class="math display">\[ X(X-1) = \sum_{i=1}^n \sum_{j\neq i} Y_{ij}. \]</span>
Now, by linearity:
<span class="math display">\[ E[X(X-1)] = \sum_{i=1}^n \sum_{j\neq i} E[Y_{ij}]. \]</span>
But <span class="math inline">\(E[Y_{ij}]\)</span> is simply the probability that tickets <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are both <span class="math inline">\(\fbox{1}\)</span>s.
This probability is <span class="math inline">\(E[Y_{ij}] = \frac{N_1^2}{N^2}\)</span>. Since there are <span class="math inline">\(n(n-1)\)</span> <span class="math inline">\(Y_{ij}\)</span>s,
<span class="math display">\[ E[X(X-1)] = n(n-1) \frac{N_1^2}{N^2}, \]</span>
which matches the answer we got in Lesson <a href="lotus.html#lotus">24</a> by more tedious means.</p>
</div>
<div id="essential-practice-15" class="section level2 unnumbered">
<h2>Essential Practice</h2>
<ol style="list-style-type: decimal">
<li><p>Each year, as part of a “Secret Santa” tradition, a group of 4 friends write their names on slips of papers and place
the slips into a hat. Each member of the group draws a name at random from the hat and must by a gift for that person. Of course, it is possible that they draw their own name, in which case they buy a gift for themselves.
What is the expected number of people who draw their own name? (<em>Hint:</em> Express this complicated
random variable as a sum of <span class="math inline">\(0/1\)</span> random variables, and use linearity of expectation.)</p></li>
<li><p>McDonald’s decides to give a Pokemon toy with every Happy Meal. Each time you buy a Happy Meal, you are equally
likely to get any one of the 6 types of Pokemon. What is the <em>expected</em> number of Happy Meals that you have to buy
until you “catch ’em all”?
(<em>Hint:</em> Express this complicated random variable as a sum of geometric random variables, and
use linearity of expectation.)</p></li>
<li><p>A group of 60 people are comparing their birthdays (as usual, assume that their
birthdays are independent, all 365 days are equally likely, etc.). Find the expected number of days
in the year on which at least two of these people were born. (<em>Hint:</em> Express this complicated random variable as
a sum of <span class="math inline">\(0/1\)</span> random variables, and use linearity of expectation.)</p></li>
</ol>
</div>
<div id="additional-practice-1" class="section level2 unnumbered">
<h2>Additional Practice</h2>
<ol style="list-style-type: decimal">
<li><p>A hash table is a commonly used data structure in computer science, allowing
for fast information retrieval. For example, suppose we want to store some
people’s phone numbers. Assume that no two of the people have the same
name. For each name <span class="math inline">\(x\)</span>, a hash function <span class="math inline">\(h\)</span> is used, where <span class="math inline">\(h(x)\)</span> is the location
to store x’s phone number. After such a table has been computed, to look up
<span class="math inline">\(x\)</span>’s phone number one just recomputes <span class="math inline">\(h(x)\)</span> and then looks up what is stored
in that location.</p>
<p>Typically, <span class="math inline">\(h\)</span> is chosen to be (pseudo)random. Suppose there are 100 people,
with each person’s phone number stored in a random location (independently),
represented by an integer between 1 and 1000. It then might happen that one
location has more than one phone number stored there, if two different people
<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> end up with the same random location for their information to be
stored.</p>
<p>Find the expected number of locations with no phone numbers stored, the
expected number with exactly one phone number, and the expected number
with more than one phone number.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lotus2d.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ev-product.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dlsun/probability/edit/master/linearity.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
